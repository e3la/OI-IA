{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "mod12-e.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMPef0Gy0C7JQ/aPTmC/Q5H",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/e3la/Organizing-Information-in-Information-Agencies/blob/master/mod12_e.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vTMyTLLCRDv8"
      },
      "source": [
        "Bias in Retrieval Systems\n",
        "=========================\n",
        "\n",
        "**Issue: Bias in Retrieval Systems**\n",
        "------------------------------------\n",
        "\n",
        "Google is a web search engine. Like other search engines, there can be bias in the results. Why are the top-ranked hits at the top? The [PageRank algorithm](https://en.wikipedia.org/wiki/PageRank) is applied, but as Ekström (2015) explained in the TED Talk, there is also a human element at play.\n",
        "\n",
        "Potential issues can be a lack of transparency in how the relevance is determined, and biases in the algorithm that rank the results. For additional background, read the Wikipedia page on algorithmic bias: [https://en.wikipedia.org/wiki/Algorithmic\\_bias](https://en.wikipedia.org/wiki/Algorithmic_bias)\n",
        "\n",
        "#### The Flawed Work of Humans\n",
        "\n",
        "Even when humans are trying to be unbiased, their work runs the risk of being flawed. Read the following short article for an example of how the anti-racism algorithms in social media platforms like Twitter and Facebook might be missing the mark. \n",
        "\n",
        "> Ghaffary, S. (2019). The algorithms that detect hate speech online are biased against black people. _Vox._ [https://www.vox.com/recode/2019/8/15/20806384/social-media-hate-speech-bias-black-african-american-facebook-twitter](https://www.vox.com/recode/2019/8/15/20806384/social-media-hate-speech-bias-black-african-american-facebook-twitter)\n",
        "\n",
        "As mentioned previously in this OER, web search engines are not the only kind of search engines. The search engines of databases used in information agencies, including the library catalog, are subject to the same bias, for similar reasons.\n",
        "\n",
        "### **![](https://missouri.instructure.com/courses/49361/files/8633254/download)Questions**\n",
        "\n",
        "*   When an online library catalog sorts search results by relevance, what criteria are used?\n",
        "\n",
        "### **Example**\n",
        "\n",
        "Library of Congress Catalog search results for keyword search, \"[bias](https://catalog.loc.gov/vwebv/search?searchArg=bias&searchCode=GKEY%5E*&searchType=0&recCount=25&sk=en_US)\"\n",
        "\n",
        "*   The Catalog information pages include a description about how relevance is determined.\n",
        "*   Relevancy ranking:\n",
        "    \n",
        "    *   Keyword Search results are displayed as a Titles List in ranked order using a special relevancy algorithm. Relevancy ranking is commonly used for most web searches, but web search engines rely on different formulas to determine relevancy. The LC Catalog uses the following ranking factors:\n",
        "        *   **Uniqueness of search terms** within the Catalog\n",
        "        *   **Proximity of search terms to each other** within the Catalog record\n",
        "        *   **Number of times a search word is present** in Catalog record fields (e.g., in a subject heading field, author field, title field)\n",
        "        *   Source: [https://catalog.loc.gov/vwebv/ui/en\\_US/htdocs/help/searchKeyword.html](https://catalog.loc.gov/vwebv/ui/en_US/htdocs/help/searchKeyword.html)\n",
        "    \n",
        "*   This transparency can give us some reassurance that there is not a built-in bias.\n",
        "\n",
        "#### **Next**\n",
        "\n",
        "_Information agencies rely heavily on retrieval systems, and increasingly, those systems are also recommender systems, relying on information about the users._\n",
        "\n",
        "* * *\n",
        "\n",
        "#### Reference\n",
        "\n",
        "Ekström, A. (2015). _The moral bias behind your search results_ \\[Video\\]. TEDxOslo. [https://www.ted.com/talks/andreas\\_ekstrom\\_the\\_moral\\_bias\\_behind\\_your\\_search\\_results](https://www.ted.com/talks/andreas_ekstrom_the_moral_bias_behind_your_search_results)"
      ]
    }
  ]
}